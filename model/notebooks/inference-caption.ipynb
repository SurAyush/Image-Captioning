{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":10442949,"sourceType":"datasetVersion","datasetId":6463756},{"sourceId":226260,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":192980,"modelId":214933}],"dockerImageVersionId":30822,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport pickle\nfrom transformers import GPT2Tokenizer, GPT2LMHeadModel\nimport skimage.io as io\nimport PIL.Image","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-11T20:18:41.595583Z","iopub.execute_input":"2025-01-11T20:18:41.595926Z","iopub.status.idle":"2025-01-11T20:18:49.037674Z","shell.execute_reply.started":"2025-01-11T20:18:41.595899Z","shell.execute_reply":"2025-01-11T20:18:49.036459Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\ndevice","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-11T20:19:49.348342Z","iopub.execute_input":"2025-01-11T20:19:49.349033Z","iopub.status.idle":"2025-01-11T20:19:49.361410Z","shell.execute_reply.started":"2025-01-11T20:19:49.348976Z","shell.execute_reply":"2025-01-11T20:19:49.360259Z"}},"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"device(type='cpu')"},"metadata":{}}],"execution_count":11},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass FeedForward(nn.Module):\n    '''Feed Forward Network for Transformer'''\n\n    def __init__(self, d_model, d_ff, dropout_ratio = 0.1):\n        super().__init__()\n        self.linear1 = nn.Linear(d_model, d_ff)\n        self.relu = nn.ReLU()\n        self.linear2 = nn.Linear(d_ff, d_model)\n        self.dropout = nn.Dropout(dropout_ratio)\n\n    def forward(self, x):\n        x = self.linear1(x)\n        x = self.relu(x)\n        x = self.linear2(x)\n        x = self.dropout(x)\n        return x\n\n\nclass MultiHeadSA(nn.Module):\n    '''Multi Head Self Attention Layer'''\n\n    def __init__(self, n_heads, d_model, input_dim):  \n        super().__init__()    \n        assert d_model % n_heads == 0 , \"Invalid head_size for the given d_model\"\n        self.n_heads = n_heads\n        self.d_model = d_model\n        self.head_size = d_model // n_heads\n        self.input_dim = input_dim\n        self.qkv_proj = nn.Linear(input_dim, 3 * d_model)\n        self.linear = nn.Linear(d_model, d_model)\n    \n    def forward(self, X, mask = None):\n\n        B, T, C = X.shape\n        assert C == self.input_dim, \"Input dimension does not match the model input dimension\"\n        qkv = self.qkv_proj(X)                                    # (B,T,3*D)\n        qkv = qkv.reshape(B, T, self.n_heads, 3 * self.d_model // self.n_heads)\n        qkv = qkv.permute(0,2,1,3)\n        q, k, v = torch.chunk(qkv, 3, dim=-1)\n        if mask is None:\n            attention_score = torch.softmax(q @ k.transpose(-2, -1) / (self.head_size ** 0.5), dim=-1)\n        else:\n            mask = mask.unsqueeze(1)  # for broadcasting\n            attention_score = torch.softmax(q @ k.transpose(-2, -1) / (self.head_size ** 0.5) + mask, dim=-1)\n        res = attention_score @ v                                       # (B,H,T,head_size)\n        res = res.permute(0,2,1,3).reshape(B, T, self.d_model)   \n        res = self.linear(res)\n\n        return res               \n\nclass EncoderLayer(nn.Module):\n    '''Single Layer of Transformer Encoder'''\n    def __init__(self, config):\n        super().__init__()\n        self.config = config\n        self.multi_head_sa = MultiHeadSA(self.config.n_heads, self.config.d_model, self.config.d_model)\n        self.feed_forward = FeedForward(self.config.d_model, self.config.d_ff, self.config.dropout)\n        self.norm1 = nn.LayerNorm(self.config.d_model)\n        self.norm2 = nn.LayerNorm(self.config.d_model)\n        self.dropout = nn.Dropout(self.config.dropout)\n\n    def forward(self, x, mask = None):\n        # ordering of layernorm is like the GPT2 paper and not like the original transformer paper\n        # layer norm before attention and feed forward\n        res = self.norm1(x)\n        res = self.multi_head_sa(res, mask)\n        res = x + self.dropout(res)          # residual connection and dropout\n        res = self.norm2(res)\n        res2 = self.feed_forward(res)\n        res = res + self.dropout(res2)\n\n        return res\n\nclass Transformer(nn.Module):\n    '''Modified Encoder Only Representation of Transformer\n        for converting CLIP embedding to GPT2 input'''\n    \n    def __init__(self, config):\n        super().__init__()\n        self.config = config\n        self.encoder_layers = nn.ModuleList([EncoderLayer(config) for _ in range(self.config.n_layers)])\n\n    def forward(self, x, mask = None):\n        for layer in self.encoder_layers:\n            x = layer(x, mask)\n        \n        return x\n    \n\nclass Mapping_Network(nn.Module):\n\n    def __init__(self, config):\n\n        super().__init__()\n        self.config = config\n        self.linear = nn.Linear(config.n_clip_emb, config.clip_length * config.d_model)        # 512 -> clip_length * d_model(768)\n        self.fixed_prefix = nn.Parameter(torch.randn(config.prefix_length, config.d_model), requires_grad=True)            # fixed prefix\n        self.transformer = Transformer(config)\n\n\n    def forward(self, x):\n        # x: (batch_size, n_clip_emb)\n        res = self.linear(x)          # (batch_size, clip_length * d_model)\n        res = res.view(res.shape[0], self.config.clip_length, self.config.d_model)        # (batch_size, clip_length, d_model)\n        prefix = self.fixed_prefix.unsqueeze(0)             # adding batch dimension\n        prefix = prefix.repeat(res.shape[0], 1, 1)          # (batch_size, prefix_length, d_model)\n        # first clip_embedding followed by fixed prefix\n        res = torch.cat((res, prefix), dim=1)               # (batch_size, prefix_length + clip_length, d_model)\n        res = self.transformer(res)                         \n        \n        return res[:,self.config.clip_length:]       \n\n\nclass CaptionModel(nn.Module):\n\n    def __init__(self,config):\n        super().__init__()\n        self.gpt = GPT2LMHeadModel.from_pretrained('gpt2')\n        self.gpt_embedding_size = self.gpt.transformer.wte.weight.shape[1]\n        self.config = config\n        self.clip_embedding_mapping = Mapping_Network(config)\n\n    def forward(self, tokens, prefix, mask):\n        cap_emb = self.gpt.transformer.wte(tokens)          # (batch_size, seq_len, embedding_size)\n        clip_emb = self.clip_embedding_mapping(prefix).view(-1,self.config.prefix_length,self.gpt_embedding_size)      # (batch_size, prefix_length, d_model_gpt2)\n        res = torch.cat((clip_emb, cap_emb), dim=1)\n        res = self.gpt(inputs_embeds = res, attention_mask = mask)\n\n        return res\n    \n    def train(self, mode = True):\n        super(CaptionModel, self).train(mode)\n        # freeze and train\n        self.gpt.eval()                      # gpt2 weights remain fixed\n        return self","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-11T20:19:50.911978Z","iopub.execute_input":"2025-01-11T20:19:50.912439Z","iopub.status.idle":"2025-01-11T20:19:51.085606Z","shell.execute_reply.started":"2025-01-11T20:19:50.912405Z","shell.execute_reply":"2025-01-11T20:19:51.084352Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"from transformers import CLIPProcessor, CLIPModel","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-11T20:19:54.297828Z","iopub.execute_input":"2025-01-11T20:19:54.298253Z","iopub.status.idle":"2025-01-11T20:19:55.463342Z","shell.execute_reply.started":"2025-01-11T20:19:54.298220Z","shell.execute_reply":"2025-01-11T20:19:55.462355Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"class Predictor():\n    \n    def __init__(self, config, path):\n        \n        self.device = torch.device(\"cpu\")\n        self.clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(device)\n        self.preprocess = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\", return_tensor='pt')\n        self.tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n        self.config = config\n        self.model = CaptionModel(self.config)\n        trained_state_dict = torch.load(path, map_location= torch.device('cpu'),  weights_only=True)\n        updated_state_dict = {k.replace(\"module.\", \"\"): v for k, v in trained_state_dict.items()}\n        self.model.load_state_dict(updated_state_dict)\n        self.model = self.model.eval()\n        self.model = self.model.to(self.device)\n\n    \n    def predict(self, image):\n        \n        image = io.imread(image)\n        pil_img = PIL.Image.fromarray(image)\n        processed_img = self.preprocess(images = pil_img, return_tensors='pt', padding=True)\n        image_tensor = processed_img['pixel_values']\n        image = image_tensor.to(self.device)\n        \n        with torch.no_grad():\n            prefix = self.clip_model.get_image_features(pixel_values = image).to(self.device, dtype=torch.float32)\n            prefix_embed = self.model.clip_embedding_mapping(prefix).reshape(1, self.config.prefix_length, -1)\n        \n        \n        return generate_beam(self.model, self.tokenizer, embed=prefix_embed)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-11T20:19:55.464695Z","iopub.execute_input":"2025-01-11T20:19:55.465373Z","iopub.status.idle":"2025-01-11T20:19:55.474182Z","shell.execute_reply.started":"2025-01-11T20:19:55.465325Z","shell.execute_reply":"2025-01-11T20:19:55.472792Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"import numpy as np","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-11T20:19:58.643675Z","iopub.execute_input":"2025-01-11T20:19:58.644072Z","iopub.status.idle":"2025-01-11T20:19:58.648917Z","shell.execute_reply.started":"2025-01-11T20:19:58.644041Z","shell.execute_reply":"2025-01-11T20:19:58.647503Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"\n    def generate_beam(model,tokenizer,embed, beam_size = 5, temperature = 1.0):\n\n        model.eval()\n        stop_token = '.'\n        stop_token_index = tokenizer.encode(stop_token)[0]\n        filter_value = -float(\"Inf\")\n        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n        entry_length = 75\n        tokens = None\n        generated_list = []\n        scores = None           # 1d tensor containing score of every prediction\n        seq_lengths = torch.ones(beam_size, device=device)             # contains seq len of each prediction\n        is_stopped = torch.zeros(beam_size, device=device, dtype=torch.bool)      # flags if stopped\n        \n\n        with torch.no_grad():\n            \n            generated = embed         # (1,prefix_length=20,d_model=768)\n            \n            for i in range(entry_length):\n\n                outputs = model.gpt(inputs_embeds=generated)\n                logits = outputs.logits\n                logits = logits[:, -1, :] \n                probs = F.softmax(logits, dim=-1)\n                out = probs.log()             # (1,vocab_size)-> initially , later (beam_size, vocab)\n\n                if scores is None:\n                    scores, next_tokens = out.topk(beam_size, -1)      # scores (1,beam_size): out , next_tokens(1,beam_size): indices\n                    generated = generated.expand(beam_size, generated.shape[1], generated.shape[2])\n                    next_tokens = next_tokens.permute(1, 0)        # making B predictions with top-b tokens\n                    scores = scores.squeeze(0)                     # removing batch-dim\n                    if tokens is None:\n                        tokens = next_tokens\n                    else:\n                        print(\"wtf\")\n                else:\n                    logits[is_stopped] = -float(np.inf)             # marks all values as -inf for every beam which is stopped\n                    logits[is_stopped, 0] = 0                       # initial element of every stopped search as 0\n                    # scores[:,None] is same as scores.reshape(beam_size,1) but it doesn't create extra memory\n                    scores_sum = scores[:, None] + logits           # scores is reshaped from 1d tensor to 2d for broadcasted addition\n                    seq_lengths[~is_stopped] += 1\n                    scores_sum_average = scores_sum / seq_lengths[:, None]           # avg: better judging parameter\n                    scores_sum_average, next_tokens = scores_sum_average.view(-1).topk(beam_size, -1)      # re-evalutaing and finding top k (beam-size) logits\n                    \n                    next_tokens_source = next_tokens // scores_sum.shape[1]                                # indices are flattened, mapping to corresponding beam\n                    seq_lengths = seq_lengths[next_tokens_source]                                          # flattened index, mapping back to original index to tokens\n                    next_tokens = next_tokens % scores_sum.shape[1]\n                    next_tokens = next_tokens.unsqueeze(1)\n                    tokens = tokens[next_tokens_source]\n                    tokens = torch.cat((tokens, next_tokens), dim=1)\n                    generated = generated[next_tokens_source]\n                    scores = scores_sum_average * seq_lengths\n                    is_stopped = is_stopped[next_tokens_source]\n                \n                \n                next_token_embed = model.gpt.transformer.wte(next_tokens.squeeze()).view(generated.shape[0], 1, -1)\n                generated = torch.cat((generated, next_token_embed), dim=1)\n                is_stopped = is_stopped + next_tokens.eq(stop_token_index).squeeze()\n                \n                if is_stopped.all():\n                    break\n                    \n        scores = scores / seq_lengths\n        output_list = tokens.cpu().numpy()\n        output_texts = [\n            tokenizer.decode(output[: int(length)])\n            for output, length in zip(output_list, seq_lengths)\n        ]\n        \n        order = scores.argsort(descending=True)\n        output_texts = [output_texts[i] for i in order]\n        \n        return output_texts\n                ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-11T20:19:59.914617Z","iopub.execute_input":"2025-01-11T20:19:59.914969Z","iopub.status.idle":"2025-01-11T20:19:59.928165Z","shell.execute_reply.started":"2025-01-11T20:19:59.914941Z","shell.execute_reply":"2025-01-11T20:19:59.926623Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"\n    def generate(model,tokenizer,embed):\n\n        model.eval()\n        stop_token = '.'\n        stop_token_index = tokenizer.encode(stop_token)[0]\n        filter_value = -float(\"Inf\")\n        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n        entry_length = 75\n        tokens = None\n        generated_list = []\n        \n\n        with torch.no_grad():\n            \n            generated = embed\n            \n            for i in range(entry_length):\n\n                outputs = model.gpt(inputs_embeds=generated)\n                logits = outputs.logits\n                logits = logits[:, -1, :]\n                probs = F.softmax(logits, dim=-1)\n                next_token = torch.multinomial(probs,1)\n                next_token_embed = model.gpt.transformer.wte(next_token)\n                if tokens is None:\n                    tokens = next_token\n                else:\n                    tokens = torch.cat((tokens, next_token), dim=1)\n                generated = torch.cat((generated, next_token_embed), dim=1)\n                if stop_token_index == next_token.item():\n                    break\n\n            output_list = list(tokens.squeeze().cpu().numpy())\n            output_text = tokenizer.decode(output_list)\n            generated_list.append(output_text)\n\n        \n        return generated_list[0]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-11T20:18:25.220248Z","iopub.execute_input":"2025-01-11T20:18:25.220718Z","iopub.status.idle":"2025-01-11T20:18:25.244235Z","shell.execute_reply.started":"2025-01-11T20:18:25.220676Z","shell.execute_reply":"2025-01-11T20:18:25.242647Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"from dataclasses import dataclass\n\n@dataclass\nclass Config():\n    epochs: int = 4\n    batch_size: int = 128\n    lr: float = 3e-5\n    warmup_steps: int = 3000\n    n_layers: int = 6\n    n_clip_emb: int = 512\n    n_heads: int = 8\n    d_model: int = 768\n    d_ff: int = 3072\n    dropout: float = 0.1\n    prefix_length: int = 20\n    clip_length: int = 10\n    dropout: float = 0.1","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-11T20:20:05.796579Z","iopub.execute_input":"2025-01-11T20:20:05.796983Z","iopub.status.idle":"2025-01-11T20:20:05.804319Z","shell.execute_reply.started":"2025-01-11T20:20:05.796949Z","shell.execute_reply":"2025-01-11T20:20:05.802964Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"path = '/kaggle/input/caption_model_epoch4/pytorch/default/1/model_epoch_4.pt'\nconfig = Config()\n\npredictor = Predictor(config,path)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-11T20:20:07.762443Z","iopub.execute_input":"2025-01-11T20:20:07.762811Z","iopub.status.idle":"2025-01-11T20:20:35.352172Z","shell.execute_reply.started":"2025-01-11T20:20:07.762778Z","shell.execute_reply":"2025-01-11T20:20:35.350954Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/4.19k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1708994973874c7fa082a4f2c1f13ffe"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/605M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"83149bb40ab44f89962945a0cc4c52f7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"preprocessor_config.json:   0%|          | 0.00/316 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"64a3b4b2082841aa97be47a4a7de7cca"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/592 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6de0e15bb0114bd0b73d0cc6c5f251ce"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/862k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5de063faba04407c878475f18e7dfb5b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/525k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b49a87845189432091568555659ef9be"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/2.22M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d9d888349b9b4ba18b2d3846b546e91a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/389 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1add79df8e6b4af49f967245055114ea"}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5821cefbc0874dcd84f49304cde10323"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"73a63f4c27614651b4dd3ab5b09d164c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b581f1760fe94b54a535a05b725135c5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"927dc6b927f14188948cdb23955ccc45"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1e0a0abc92d04a81b3d157a53f62ef45"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"20f2e44cf2a44f2b97e798e2adf8329f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d12e297cccf4487f8574d59b8e6aab91"}},"metadata":{}}],"execution_count":18},{"cell_type":"raw","source":"","metadata":{}},{"cell_type":"code","source":"image_path = '/kaggle/input/test-image/test4.jpg'\n\nres = predictor.predict(image_path)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-11T20:20:38.673564Z","iopub.execute_input":"2025-01-11T20:20:38.674349Z","iopub.status.idle":"2025-01-11T20:20:46.990621Z","shell.execute_reply.started":"2025-01-11T20:20:38.674308Z","shell.execute_reply":"2025-01-11T20:20:46.989156Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"res","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-11T20:20:48.657477Z","iopub.execute_input":"2025-01-11T20:20:48.657859Z","iopub.status.idle":"2025-01-11T20:20:48.665294Z","shell.execute_reply.started":"2025-01-11T20:20:48.657826Z","shell.execute_reply":"2025-01-11T20:20:48.663979Z"}},"outputs":[{"execution_count":20,"output_type":"execute_result","data":{"text/plain":"['A young person sitting in front with a laptop.',\n 'A young person sitting in front of  a desk with a computer on top.',\n 'A young person sitting in front of  a desk with computer.',\n 'A young person sitting in front of  a desk with a computer.',\n 'A young person sitting in front of  a desk with a computer on top of it.']"},"metadata":{}}],"execution_count":20},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}