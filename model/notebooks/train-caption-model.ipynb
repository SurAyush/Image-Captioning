{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":10334773,"sourceType":"datasetVersion","datasetId":6399261},{"sourceId":220729,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":188254,"modelId":210287}],"dockerImageVersionId":30823,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport pickle","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-06T13:09:28.347566Z","iopub.execute_input":"2025-01-06T13:09:28.347881Z","iopub.status.idle":"2025-01-06T13:09:33.475341Z","shell.execute_reply.started":"2025-01-06T13:09:28.347850Z","shell.execute_reply":"2025-01-06T13:09:33.474438Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\ndevice","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-06T13:11:55.088806Z","iopub.execute_input":"2025-01-06T13:11:55.089096Z","iopub.status.idle":"2025-01-06T13:11:55.175447Z","shell.execute_reply.started":"2025-01-06T13:11:55.089075Z","shell.execute_reply":"2025-01-06T13:11:55.174772Z"}},"outputs":[{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"device(type='cuda')"},"metadata":{}}],"execution_count":2},{"cell_type":"code","source":"file_loc = '/kaggle/input/clip-emb-coco-2017/final_embeddings.pkl'\n\nwith open(file_loc, 'rb') as f:\n    data = pickle.load(f)\n\nclip_embeddings = data[\"clip_embedding\"]  # The concatenated image embeddings\ncaptions = data[\"captions\"]  # The list of captions\n\nprint(captions[:5])\nprint(len(clip_embeddings))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-06T13:11:57.758167Z","iopub.execute_input":"2025-01-06T13:11:57.758457Z","iopub.status.idle":"2025-01-06T13:12:05.808711Z","shell.execute_reply.started":"2025-01-06T13:11:57.758434Z","shell.execute_reply":"2025-01-06T13:12:05.807931Z"}},"outputs":[{"name":"stdout","text":"['an old man checking his cell phone by a building ', 'A simple bathroom with black and orange towels.', 'A kitchen with a wooden center island under two lights.', 'A large, spacious kitchen with an island in the middle.', 'A center island sitting in the middle of a kitchen.']\n391753\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"from dataclasses import dataclass\n\n@dataclass\nclass Config():\n    epochs: int = 2\n    batch_size: int = 32\n    lr: float = 1e-4\n    warmup_steps: int = 3000\n    n_layers: int = 6\n    n_clip_emb: int = 512\n    n_heads: int = 8\n    d_model: int = 768\n    d_ff: int = 3072\n    dropout: float = 0.1\n    prefix_length: int = 20\n    clip_length: int = 10\n    dropout: float = 0.1","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-06T13:22:12.775512Z","iopub.execute_input":"2025-01-06T13:22:12.775843Z","iopub.status.idle":"2025-01-06T13:22:12.781315Z","shell.execute_reply.started":"2025-01-06T13:22:12.775817Z","shell.execute_reply":"2025-01-06T13:22:12.780441Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import Dataset\nfrom transformers import GPT2Tokenizer\nimport pickle\n\nclass Caption_Dataset(Dataset):\n\n    def __init__(self, file_path, prefix_length, extract_from_file = False):\n        self.tokenizer = GPT2Tokenizer.from_pretrained('gpt2')      # gpt2 tokenizer from HF\n        self.prefix_length = prefix_length\n        \n        # clip embedding & captions file \n        with open(file_path, 'rb') as f:\n            data = pickle.load(f)\n        \n        self.prefixes = data[\"clip_embedding\"]\n        self.captions = data[\"captions\"]\n\n        self.captions_tokens = []        # to store list of tokenised captions\n        max_seq_len = 0\n\n        if extract_from_file:          # if tokenisation is already done, load from file\n            with open('/kaggle/working/caption_tokens.pkl', 'rb') as f:\n                self.captions_tokens, self.max_seq_len = pickle.load(f)\n        else:\n            for caption in self.captions:\n                self.captions_tokens.append(torch.tensor(self.tokenizer.encode(caption), dtype=torch.int64))           # storing tokenised captions\n                max_seq_len = max(max_seq_len, self.captions_tokens[-1].shape[0])    \n\n            with open(\"/kaggle/working/caption_tokens.pkl\", 'wb') as f:\n                pickle.dump([self.captions_tokens, max_seq_len], f)\n            \n            self.max_seq_len = max_seq_len\n\n    def __len__(self):\n        return len(self.captions)\n    \n    def pad_mask(self,idx):\n\n        tokens = self.captions_tokens[idx]\n        padding = self.max_seq_len - tokens.shape[0]\n        if padding > 0:\n            tokens = torch.cat((tokens, torch.zeros(padding, dtype=torch.int64) - 1))          # padding with -1 tokens\n            self.captions_tokens[idx] = tokens\n        elif padding < 0:\n            tokens = tokens[:self.max_seq_len]          # trimming the tokens\n            self.captions_tokens[idx] = tokens\n\n        mask = tokens.ge(0)              # assigns false to -1 tokens  \n        tokens[~mask] = 0                # assigns 0 to false (-1) \n        mask = mask.float()\n        # prefix should always be considered in attention mechanism\n        mask = torch.cat((torch.ones(self.prefix_length), mask), dim=0)     # adding prefix length to mask\n        \n        return tokens, mask\n\n    def __getitem__(self, idx):\n        tokens, mask = self.pad_mask(idx)\n        return tokens, mask, self.prefixes[idx]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-06T13:12:22.613053Z","iopub.execute_input":"2025-01-06T13:12:22.613430Z","iopub.status.idle":"2025-01-06T13:12:24.638975Z","shell.execute_reply.started":"2025-01-06T13:12:22.613399Z","shell.execute_reply":"2025-01-06T13:12:24.638309Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"class FeedForward(nn.Module):\n    '''Feed Forward Network for Transformer'''\n\n    def __init__(self, d_model, d_ff, dropout_ratio = 0.1):\n        super().__init__()\n        self.linear1 = nn.Linear(d_model, d_ff)\n        self.relu = nn.ReLU()\n        self.linear2 = nn.Linear(d_ff, d_model)\n        self.dropout = nn.Dropout(dropout_ratio)\n\n    def forward(self, x):\n        x = self.linear1(x)\n        x = self.relu(x)\n        x = self.linear2(x)\n        x = self.dropout(x)\n        return x\n\n\nclass MultiHeadSA(nn.Module):\n    '''Multi Head Self Attention Layer'''\n\n    def __init__(self, n_heads, d_model, input_dim):  \n        super().__init__()    \n        assert d_model % n_heads == 0 , \"Invalid head_size for the given d_model\"\n        self.n_heads = n_heads\n        self.d_model = d_model\n        self.head_size = d_model // n_heads\n        self.input_dim = input_dim\n        self.qkv_proj = nn.Linear(input_dim, 3 * d_model)\n        self.linear = nn.Linear(d_model, d_model)\n    \n    def forward(self, X, mask = None):\n\n        B, T, C = X.shape\n        assert C == self.input_dim, \"Input dimension does not match the model input dimension\"\n        qkv = self.qkv_proj(X)                                    # (B,T,3*D)\n        qkv = qkv.reshape(B, T, self.n_heads, 3 * self.d_model // self.n_heads)\n        qkv = qkv.permute(0,2,1,3)\n        q, k, v = torch.chunk(qkv, 3, dim=-1)\n        if mask is None:\n            attention_score = torch.softmax(q @ k.transpose(-2, -1) / (self.head_size ** 0.5), dim=-1)\n        else:\n            mask = mask.unsqueeze(1)  # for broadcasting\n            attention_score = torch.softmax(q @ k.transpose(-2, -1) / (self.head_size ** 0.5) + mask, dim=-1)\n        res = attention_score @ v                                       # (B,H,T,head_size)\n        res = res.permute(0,2,1,3).reshape(B, T, self.d_model)   \n        res = self.linear(res)\n\n        return res               \n\nclass EncoderLayer(nn.Module):\n    '''Single Layer of Transformer Encoder'''\n    def __init__(self, config):\n        super().__init__()\n        self.config = config\n        self.multi_head_sa = MultiHeadSA(self.config.n_heads, self.config.d_model, self.config.d_model)\n        self.feed_forward = FeedForward(self.config.d_model, self.config.d_ff, self.config.dropout)\n        self.norm1 = nn.LayerNorm(self.config.d_model)\n        self.norm2 = nn.LayerNorm(self.config.d_model)\n        self.dropout = nn.Dropout(self.config.dropout)\n\n    def forward(self, x, mask = None):\n        # ordering of layernorm is like the GPT2 paper and not like the original transformer paper\n        # layer norm before attention and feed forward\n        res = self.norm1(x)\n        res = self.multi_head_sa(res, mask)\n        res = x + self.dropout(res)          # residual connection and dropout\n        res = self.norm2(res)\n        res2 = self.feed_forward(res)\n        res = res + self.dropout(res2)\n\n        return res\n\nclass Transformer(nn.Module):\n    '''Modified Encoder Only Representation of Transformer\n        for converting CLIP embedding to GPT2 input'''\n    \n    def __init__(self, config):\n        super().__init__()\n        self.config = config\n        self.encoder_layers = nn.ModuleList([EncoderLayer(config) for _ in range(self.config.n_layers)])\n\n    def forward(self, x, mask = None):\n        for layer in self.encoder_layers:\n            x = layer(x, mask)\n        \n        return x\n    ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-06T13:12:35.221239Z","iopub.execute_input":"2025-01-06T13:12:35.221580Z","iopub.status.idle":"2025-01-06T13:12:35.233631Z","shell.execute_reply.started":"2025-01-06T13:12:35.221547Z","shell.execute_reply":"2025-01-06T13:12:35.232941Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"class Mapping_Network(nn.Module):\n\n    def __init__(self, config):\n\n        super().__init__()\n        self.config = config\n        self.linear = nn.Linear(config.n_clip_emb, config.clip_length * config.d_model)        # 512 -> clip_length * d_model(768)\n        self.fixed_prefix = nn.Parameter(torch.randn(config.prefix_length, config.d_model), requires_grad=True)            # fixed prefix\n        self.transformer = Transformer(config)\n\n\n    def forward(self, x):\n        # x: (batch_size, n_clip_emb)\n        res = self.linear(x)          # (batch_size, clip_length * d_model)\n        res = res.view(res.shape[0], self.config.clip_length, self.config.d_model)        # (batch_size, clip_length, d_model)\n        prefix = self.fixed_prefix.unsqueeze(0)             # adding batch dimension\n        prefix = prefix.repeat(res.shape[0], 1, 1)          # (batch_size, prefix_length, d_model)\n        # first clip_embedding followed by fixed prefix\n        res = torch.cat((res, prefix), dim=1)               # (batch_size, prefix_length + clip_length, d_model)\n        res = self.transformer(res)                         \n        \n        return res[:,self.config.clip_length:]       \n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-06T13:12:38.128979Z","iopub.execute_input":"2025-01-06T13:12:38.129291Z","iopub.status.idle":"2025-01-06T13:12:38.135086Z","shell.execute_reply.started":"2025-01-06T13:12:38.129254Z","shell.execute_reply":"2025-01-06T13:12:38.134314Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"from transformers import GPT2LMHeadModel\n\nclass CaptionModel(nn.Module):\n\n    def __init__(self,config):\n        super().__init__()\n        self.gpt = GPT2LMHeadModel.from_pretrained('gpt2')\n        self.gpt_embedding_size = self.gpt.transformer.wte.weight.shape[1]\n        self.config = config\n        self.clip_embedding_mapping = Mapping_Network(config)\n\n    def forward(self, tokens, prefix, mask):\n        cap_emb = self.gpt.transformer.wte(tokens)          # (batch_size, seq_len, embedding_size)\n        clip_emb = self.clip_embedding_mapping(prefix).view(-1,self.config.prefix_length,self.gpt_embedding_size)      # (batch_size, prefix_length, d_model_gpt2)\n        res = torch.cat((clip_emb, cap_emb), dim=1)\n        res = self.gpt(inputs_embeds = res, attention_mask = mask, return_dict = True)\n\n        return res.logits\n    \n    def train(self, mode = True):\n        super(CaptionModel, self).train(mode)\n        # freeze and train\n        self.gpt.eval()                      # gpt2 weights remain fixed\n        return self","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-06T13:12:40.136694Z","iopub.execute_input":"2025-01-06T13:12:40.137008Z","iopub.status.idle":"2025-01-06T13:12:41.523838Z","shell.execute_reply.started":"2025-01-06T13:12:40.136988Z","shell.execute_reply":"2025-01-06T13:12:41.523148Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"from torch.utils.data import DataLoader\nfrom transformers import AdamW, get_linear_schedule_with_warmup\nfrom tqdm import tqdm \n\nconfig = Config()\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nfile_path = '/kaggle/input/clip-emb-coco-2017/final_embeddings.pkl'\n\nmodel = CaptionModel(config)\nmodel = nn.DataParallel(model)         # Use multiple GPUs\n# Move model to GPU\nmodel = model.to(device)\n\nmodel.train()\n\ndataset = Caption_Dataset(file_path, prefix_length = config.prefix_length, extract_from_file = True)\ntrain_loader = DataLoader(dataset, batch_size=config.batch_size, shuffle=True)\noptimizer = AdamW(model.parameters(), lr=config.lr)\nscheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=config.warmup_steps, num_training_steps=config.epochs * len(train_loader))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-06T13:22:38.812626Z","iopub.execute_input":"2025-01-06T13:22:38.812940Z","iopub.status.idle":"2025-01-06T13:23:11.049771Z","shell.execute_reply.started":"2025-01-06T13:22:38.812914Z","shell.execute_reply":"2025-01-06T13:23:11.049097Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"len(train_loader)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-06T13:13:33.488683Z","iopub.execute_input":"2025-01-06T13:13:33.489147Z","iopub.status.idle":"2025-01-06T13:13:33.494136Z","shell.execute_reply.started":"2025-01-06T13:13:33.489120Z","shell.execute_reply":"2025-01-06T13:13:33.493423Z"}},"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"12243"},"metadata":{}}],"execution_count":10},{"cell_type":"code","source":"loss_hist = []","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-06T13:13:35.356091Z","iopub.execute_input":"2025-01-06T13:13:35.356401Z","iopub.status.idle":"2025-01-06T13:13:35.359773Z","shell.execute_reply.started":"2025-01-06T13:13:35.356375Z","shell.execute_reply":"2025-01-06T13:13:35.359009Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"model.load_state_dict(torch.load('/kaggle/input/caption_till_epch2/pytorch/default/1/model_epoch_2.pt'))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-06T13:23:30.841258Z","iopub.execute_input":"2025-01-06T13:23:30.841598Z","iopub.status.idle":"2025-01-06T13:23:35.875671Z","shell.execute_reply.started":"2025-01-06T13:23:30.841563Z","shell.execute_reply":"2025-01-06T13:23:35.874873Z"}},"outputs":[{"name":"stderr","text":"<ipython-input-16-8aa4bb244934>:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  model.load_state_dict(torch.load('/kaggle/input/caption_till_epch2/pytorch/default/1/model_epoch_2.pt'))\n","output_type":"stream"},{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"<All keys matched successfully>"},"metadata":{}}],"execution_count":16},{"cell_type":"code","source":"for epoch in range(2,4):\n    \n        print(f\"Epoch: {epoch}\")\n        progress = tqdm(total=len(train_loader), desc=\"Training\", leave = False)\n    \n        for idx, (tokens, mask, prefix) in enumerate(train_loader):\n\n            model.zero_grad()\n            tokens, mask, prefix = tokens.to(device), mask.to(device), prefix.to(device, dtype=torch.float32)\n            outputs = model(tokens, prefix, mask)      # (B, prefix_length + max_seq_length, d_model)\n            logits = outputs[:, config.prefix_length - 1:-1]             # (B, max_seq_length, vocab_size)\n            # only consider the output for caption tokens\n\n            # calculating loss\n            loss = F.cross_entropy(logits.reshape(-1, logits.shape[-1]), tokens.flatten(), ignore_index=0)    # ignore_index to ignore loss for padding tokens\n            loss.backward()\n            optimizer.step()       # gradient descent\n            scheduler.step()       # lr decay\n\n            optimizer.zero_grad()\n            \n            # updating progress bar\n            progress.set_postfix(loss=loss.item())\n            progress.update()\n\n            loss_hist.append(loss.item())\n\n            if (idx+1)%3000==0:\n                torch.save(model.state_dict(), f'model_epoch:{epoch+1}:{idx+1}.pt')\n                with open(f'loss_hist:{epoch+1}:{idx+1}.pkl', 'wb') as f:\n                    pickle.dump(loss_hist, f)\n                loss_hist = []\n\n        progress.close()\n        \n        torch.save(model.state_dict(), f\"model_epoch_{epoch+1}.pt\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-06T13:24:26.796928Z","iopub.execute_input":"2025-01-06T13:24:26.797197Z"}},"outputs":[{"name":"stdout","text":"Epoch: 2\n","output_type":"stream"},{"name":"stderr","text":"Training:  43%|████▎     | 5210/12243 [1:53:46<2:33:43,  1.31s/it, loss=1.76]","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"print(\"test\")","metadata":{"trusted":true,"execution":{"execution_failed":"2025-01-04T19:54:50.356Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}